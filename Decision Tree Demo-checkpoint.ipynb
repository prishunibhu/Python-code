{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from 25 Sep - Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discret input , Discrete output:\n",
    "\n",
    "Classification using Decision tree\n",
    "\n",
    "Core idea=At each level, choose an attribute that gives the biggest information gain.\n",
    "\n",
    "Building a decision tree is Non polynomial ( time is high) - computational complexity grows exponential - NP complete.\n",
    "\n",
    "Multiple decision tree algos exist- Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "1)Entropy - Measure of randomness. \n",
    "\n",
    "Entropy is low  there is an order or pattern . and entropy is always > 0 E(S) < 1 for binary classifier.\n",
    "\n",
    "When the probability is 1/2 , then the entrophy is maximum since we have likelihood of the events occuring is equal.\n",
    "\n",
    "when the prob is high/low  , then entrophy is low.\n",
    "\n",
    "\n",
    "\n",
    "This characterizes impurity/purity of of data\n",
    "\n",
    "E(S) = - [sum [p(xi) log p(xi)]]\n",
    "\n",
    "Other measures for purity/impurity\n",
    "\n",
    "Gini index = 1- sum(p(i/t)^2) and the sum is for the number of classes. . \n",
    "Gini index should be minimum for choosing the root node.\n",
    "\n",
    "Mis classification error = 1- max[p(i/t)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Root node - Max information gain is selected as root node. This is called reduction in entrophy.\n",
    "\n",
    "IG(S,A) = E(S) - sum[ SV/S Entropy(SV)]\n",
    "\n",
    "Feature with highest IG is selected as root node and then other nodes are choosen iteratively with high IG.\n",
    "\n",
    "Drawbacks:\n",
    "    \n",
    "The algo is sub optimal because I do not do reconcilation. I take one node as best, I proceed with it and do not go back to \n",
    "reconsider it.\n",
    "\n",
    "ID3 algo builds short trees or simple hypothesis.\n",
    "\n",
    "We need to determine the possible features required for classification whether it is will be suitable for the node classifcation.\n",
    "\n",
    "Pruning:\n",
    "    \n",
    "Building the tree completely and then cut down the nodes when it is does not give desired results. Cross validation method is one\n",
    "\n",
    "Discrete input , Real output:\n",
    "    \n",
    "We choose the nodes with having reduction in variance.\n",
    "\n",
    "\n",
    "Real input , Discrete output:\n",
    "\n",
    "\n",
    "Real input, real output:\n",
    "\n",
    "Usually Linear regression/ SVM is preferred.\n",
    "\n",
    "Average is used for tree building.\n",
    "\n",
    "we can compute MAE,MSE,RMSE. Minimize loss functions is the key for decision tree making.\n",
    "\n",
    "Summary of decision tree:\n",
    "    \n",
    "IT uses greedy approach goes behind max information gain.\n",
    "NP complete.\n",
    "Can overfit easily.\n",
    "\n",
    "how to avoid overfitting:\n",
    "    \n",
    "overfitting- model that performs well in train data and fails in test.\n",
    "\n",
    "holdout method- choose a model to do a performance tuning on the train data itself. split it again to see if it performs well. \n",
    "\n",
    "Kfold validation - split dataset into k folds. do train test in each dataset and determine the performance measures.\n",
    "\n",
    "Stratified k fold validation- use a specified strategy to split the data into different folds\n",
    "\n",
    "Rolling k fold - used in time series. iteration 1 - tr 1 , ts2, it2-> tr1,tr2 - ts3 and goes on.\n",
    "\n",
    "\n",
    "output of model:\n",
    "    \n",
    "Every model has the expected prediction error:\n",
    "\n",
    "This is the white noise which cannot be predicted because it can be random.\n",
    "\n",
    "IT is the diff\n",
    "\n",
    "\n",
    "1)Variable of the residuals is called noise.- errors in the best model is called noise.\n",
    "\n",
    "2)Difference between the average of the best model(train model) and the average of the predicted model is called bias squared.\n",
    "\n",
    "\n",
    "3)Variance of the model - variation in the developed model.\n",
    "\n",
    "error - noise + bias squared + variance.\n",
    "\n",
    "\n",
    "The models should be built so that we have less bias/less variance- bias variancce trade off.\n",
    "\n",
    "\n",
    "Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. \n",
    "The node probability can be calculated by the number of samples that reach the node, \n",
    "divided by the total number of samples.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     crim    zn  indus  chas    nox     rm   age     dis  rad  \\\n",
       "0           1  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1   \n",
       "1           2  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2   \n",
       "2           3  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2   \n",
       "3           4  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3   \n",
       "4           5  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3   \n",
       "\n",
       "   tax  ptratio   black  lstat  medv  \n",
       "0  296     15.3  396.90   4.98  24.0  \n",
       "1  242     17.8  396.90   9.14  21.6  \n",
       "2  242     17.8  392.83   4.03  34.7  \n",
       "3  222     18.7  394.63   2.94  33.4  \n",
       "4  222     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Boston=pd.read_csv(\"Boston.csv\")\n",
    "df_Boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rm  lstat   age\n",
      "42   6.169   5.81   6.6\n",
      "58   6.145   6.86  29.2\n",
      "385  5.277  30.81  98.1\n",
      "78   6.232  12.34  53.7\n",
      "424  5.565  17.16  70.6\n",
      "..     ...    ...   ...\n",
      "255  5.876   9.25  19.1\n",
      "72   6.065   5.52   7.8\n",
      "396  6.405  19.37  96.0\n",
      "235  6.086  10.88  61.5\n",
      "37   5.850   8.77  41.5\n",
      "\n",
      "[404 rows x 3 columns]      medv\n",
      "42   25.3\n",
      "58   23.3\n",
      "385   7.2\n",
      "78   21.2\n",
      "424  11.7\n",
      "..    ...\n",
      "255  20.9\n",
      "72   22.8\n",
      "396  12.5\n",
      "235  24.0\n",
      "37   21.0\n",
      "\n",
      "[404 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "x=pd.DataFrame(df_Boston[['rm','lstat','age']])\n",
    "y=pd.DataFrame(df_Boston['medv'])\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n",
    "print(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_regressor = DecisionTreeRegressor()\n",
    "decision_regressor.fit(x_train,y_train)\n",
    "y_pred=decision_regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 6.43598949535452\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(y_pred,y_test)\n",
    "rmse=math.sqrt(mse)\n",
    "print(\"RMSE\",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree as classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "df_Iris=pd.read_csv(\"Iris.csv\")\n",
    "x=df_Iris.iloc[:,:-1]\n",
    "y=df_Iris.iloc[:,-1]\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "[[11  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0  6]]\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-virginica'] 14         Iris-setosa\n",
      "98     Iris-versicolor\n",
      "75     Iris-versicolor\n",
      "16         Iris-setosa\n",
      "131     Iris-virginica\n",
      "56     Iris-versicolor\n",
      "141     Iris-virginica\n",
      "44         Iris-setosa\n",
      "29         Iris-setosa\n",
      "120     Iris-virginica\n",
      "94     Iris-versicolor\n",
      "5          Iris-setosa\n",
      "102     Iris-virginica\n",
      "51     Iris-versicolor\n",
      "78     Iris-versicolor\n",
      "42         Iris-setosa\n",
      "92     Iris-versicolor\n",
      "66     Iris-versicolor\n",
      "31         Iris-setosa\n",
      "35         Iris-setosa\n",
      "90     Iris-versicolor\n",
      "84     Iris-versicolor\n",
      "77     Iris-versicolor\n",
      "40         Iris-setosa\n",
      "125     Iris-virginica\n",
      "99     Iris-versicolor\n",
      "33         Iris-setosa\n",
      "19         Iris-setosa\n",
      "73     Iris-versicolor\n",
      "146     Iris-virginica\n",
      "Name: species, dtype: object\n"
     ]
    }
   ],
   "source": [
    "classifier=DecisionTreeClassifier()\n",
    "classifier.fit(x_train,y_train)\n",
    "y_pred=classifier.predict(x_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(y_pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
